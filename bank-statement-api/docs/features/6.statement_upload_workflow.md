Implement a file upload and processing workflow with two main endpoints: analyze_file and upload_file. The goal is to allow users to upload bank statement files, preview the parsed content, adjust mappings, and finalize the upload by writing normalized transactions to the database.

Note: Some of the classes are already implemented in the codebase. Just check if they are correct and add the missing ones.

Overall Flow

There are two endpoints:
	‚Ä¢	analyze_file(file: bytes, filename: str) -> FileAnalysisResponse
	‚Ä¢	upload_file(spec: UploadFileSpec) -> UploadResult

‚∏ª

Classes and Responsibilities

StatementReader
	‚Ä¢	Converts file content into a pandas.DataFrame
	‚Ä¢	Uses a FileTypeDetector to determine the file type (CSV, Excel, etc.)
	‚Ä¢	Dispatches to the correct parser (e.g. CsvParser, ExcelParser)

FileTypeDetector
	‚Ä¢	Inspects file content or file name to determine the format (e.g., ‚Äú.csv‚Äù ‚Üí csv)

StatementSchema
	‚Ä¢	Contains:
	‚Ä¢	column_mapping: mapping from original to normalized column names
	‚Ä¢	metadata (e.g., delimiter, date format, currency)
	‚Ä¢	source_id (optional)

ColumnNormalizer
	‚Ä¢	Suggests normalized column names like ‚Äúdate‚Äù, ‚Äúamount‚Äù, ‚Äúdescription‚Äù, etc.
	‚Ä¢	Works on a DataFrame with raw column names

TransactionCleaner
	‚Ä¢	Normalizes column values (date format, amount parsing, etc.)
	‚Ä¢	Drops blank or malformed rows

TransactionsBuilder
	‚Ä¢	Converts a cleaned DataFrame to a list of Transaction objects

‚∏ª

Endpoint: analyze_file
	1.	Save the uploaded file to the database (statements table), and get a statement_id
	2.	Use StatementReader to read the file into a DataFrame
	3.	Calculate a hash string from the DataFrame‚Äôs column names (e.g., SHA256 of joined headers)
	4.	Query the statement_schemas table for a matching StatementSchema using this hash
	5.	If not found:
	‚Ä¢	Use ColumnNormalizer to create a default column mapping
	‚Ä¢	Use TransactionCleaner to normalize the sample rows
	‚Ä¢	Create a new StatementSchema and store it in statement_schemas
	6.	Return a FileAnalysisResponse:
	‚Ä¢	The StatementSchema
	‚Ä¢	A preview of the first 10 cleaned and mapped rows

‚∏ª

Endpoint: upload_file
	1.	Load file content from the statements table using the statement_id from the UploadFileSpec
	2.	Use StatementReader to load it into a DataFrame
	3.	Calculate the column hash again
	4.	Create or update the associated StatementSchema in the statement_schemas table
	5.	Apply the column mapping from the uploaded StatementSchema to the DataFrame
	6.	Use TransactionCleaner to normalize all column values (dates, amounts, etc.)
	7.	Use TransactionsBuilder to generate a list of Transaction objects from the cleaned DataFrame
	8.	Persist transactions to the database (transactions table)

‚∏ª

Data Models

FileAnalysisResponse

```python
class FileAnalysisResponse:
    statement_schema: StatementSchema
    preview_rows: List[Dict[str, Any]]
```

UploadFileSpec

```python
class UploadFileSpec:
    statement_id: str
    schema: StatementSchema
```

StatementSchema
```python
class StatementSchema:
    id: str
    source_id: Optional[int]
    file_type: FileType
    column_mapping: ColumnMapping
```

Implement this using clean, testable, reusable code. Use type hints and structure logic around these responsibilities. Don‚Äôt worry about authentication or UI; focus entirely on the data transformation and processing layers.


üß± Development Plan Prompt

You will help implement a file upload workflow for transforming bank statements into transactions. Follow this process strictly:
	1.	Implement one class at a time
	2.	For each class:
	‚Ä¢	First, write a unit test (just the test, no implementation)
	‚Ä¢	Wait for confirmation
	‚Ä¢	Then write the class implementation
	‚Ä¢	Wait for confirmation
	3.	Repeat this until all classes are implemented

Follow the class-by-class plan below.

‚∏ª

üìã Step-by-Step Class Implementation Plan

1. FileTypeDetector
	‚Ä¢	Purpose: Detect file type from filename or raw bytes
	‚Ä¢	Test: Detect .csv, .xlsx, unsupported types
	‚Ä¢	Input: filename: str or file: bytes
	‚Ä¢	Output: "csv", "excel", etc.

‚∏ª

2. StatementReader
	‚Ä¢	Purpose: Parse the uploaded file into a pandas.DataFrame
	‚Ä¢	Dependencies: Uses FileTypeDetector, and internally selects:
	‚Ä¢	CsvParser
	‚Ä¢	ExcelParser
	‚Ä¢	Test: Given a file and type, ensure it returns a correct DataFrame

‚∏ª

3. ColumnNormalizer
	‚Ä¢	Purpose: Map raw column names to canonical ones (date, amount, etc.)
	‚Ä¢	Test: Map sample column headers to expected canonical names
	‚Ä¢	Output: Dict[str, str] ‚Üí normalized column mapping

‚∏ª

4. TransactionCleaner
	‚Ä¢	Purpose: Normalize row values (dates, amounts), drop bad rows
	‚Ä¢	Test: Given raw data, return cleaned data with correct types
	‚Ä¢	Output: pd.DataFrame cleaned and normalized

‚∏ª

5. StatementSchema
	‚Ä¢	Purpose: Encapsulate how a file should be interpreted (column mapping, etc.)
	‚Ä¢	Test: Not needed
	‚Ä¢	Fields: column_mapping, delimiter, date_format, source_id, etc.

‚∏ª

6. TransactionsBuilder
	‚Ä¢	Purpose: Convert a normalized DataFrame into a list of Transaction objects
	‚Ä¢	Test: Given a DataFrame with cleaned values, return list of Transactions

‚∏ª

7. FileAnalysisService
	‚Ä¢	Purpose: Logic for analyze_file endpoint
	‚Ä¢	Test: Given a file, store it, analyze headers, return a FileAnalysisResponse

‚∏ª

8. UploadFileService
	‚Ä¢	Purpose: Logic for upload_file endpoint
	‚Ä¢	Test: Given a statement ID and schema, load file, normalize, write transactions

‚∏ª

9. Database Integration (optional final step)
	‚Ä¢	Tables:
	‚Ä¢	statements (id, content, filename, etc.)
	‚Ä¢	statement_schemas (id, column_hash, schema data)
	‚Ä¢	transactions (id, statement_id, date, amount, description, etc.)
	‚Ä¢	Test reading/writing with stubbed DB layer or mock repository classes

‚∏ª

üö¶ Start Here

Begin with FileAnalysisService
	1.	Write the test first.
	2.	Wait for confirmation.
	3.	Then implement the class.